{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_ass4_prev.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Computer-Vision-IIITH-2021/assignment-4-Avinash2468/blob/main/CV_ass4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu1ecangazdW"
      },
      "source": [
        "# Importing libaries and defining all the functions we will use in our code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4s3QLYNuVAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62d9898-93a6-4836-8c0e-fac261f194a0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif\n",
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "###############\n",
        "\n",
        "def integral_image(img): #To calculate the integral image.\n",
        "    \n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "    integ_img = np.zeros_like(img).astype(np.uint32)\n",
        "    \n",
        "    for i in range(width):\n",
        "        for j in range(height):\n",
        "            \n",
        "            if i-1<0 and j-1<0:\n",
        "                integ_img[i,j] = img[i,j]\n",
        "                \n",
        "            elif i-1<0:\n",
        "                integ_img[i,j] = integ_img[i,j-1] + img[i,j]\n",
        "            \n",
        "            elif j-1<0:\n",
        "                integ_img[i,j] = integ_img[i-1,j] + img[i,j]\n",
        "            \n",
        "            else:\n",
        "                integ_img[i,j] = integ_img[i-1,j] + integ_img[i,j-1] - integ_img[i-1,j-1] + img[i,j]\n",
        "    \n",
        "    return integ_img\n",
        "\n",
        "###############\n",
        "\n",
        "class sum_rectangle_region: # Finding the value of a feature\n",
        "    def __init__(self,i,j,w,h):\n",
        "        self.i = i\n",
        "        self.j = j\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "        \n",
        "    def compute_area(self,integ_img):\n",
        "        \n",
        "        top_left = integ_img[self.i,self.j]\n",
        "        top_right = integ_img[self.i,self.j+self.w]\n",
        "        bottom_left = integ_img[self.i+self.h,self.j]\n",
        "        bottom_right = integ_img[self.i+self.h,self.j+self.w]\n",
        "        final_area = bottom_right + top_left -(top_right+bottom_left)\n",
        "        \n",
        "        return final_area\n",
        "\n",
        "###############\n",
        "\n",
        "def features(size): # Getting the features\n",
        "    features_list = []\n",
        "    \n",
        "    for i in range(size):\n",
        "        for j in range(size):       \n",
        "            h = 1\n",
        "            while i+h<size:\n",
        "                w = 1\n",
        "                while j+w<size:\n",
        "                    \n",
        "                    ########### Edge Features\n",
        "                    \n",
        "                    if i+2*h<size: # Vertically stacked rectangles\n",
        "                        neg_reg = sum_rectangle_region(i,j,w,h)\n",
        "                        pos_reg = sum_rectangle_region(i+h,j,w,h)\n",
        "                        features_list.append([[pos_reg],[neg_reg]])\n",
        "                        \n",
        "                    if j+2*w<size: # Horizontally stacked rectangles\n",
        "                        neg_reg = sum_rectangle_region(i,j,w,h)\n",
        "                        pos_reg = sum_rectangle_region(i,j+w,w,h)\n",
        "                        features_list.append([[pos_reg],[neg_reg]])\n",
        "                        \n",
        "                    ########### Line Features\n",
        "                    \n",
        "                    if i+3*h< size: # Vertically stacked rectangles\n",
        "                        neg_reg1 = sum_rectangle_region(i,j,w,h)\n",
        "                        neg_reg2 = sum_rectangle_region(i+2*h,j,w,h)\n",
        "                        pos_reg = sum_rectangle_region(i+h,j,w,h)\n",
        "                        features_list.append([[pos_reg],[neg_reg1,neg_reg2]])\n",
        "                        \n",
        "                    if j+3*w<size: # Horizontally stacked rectangles\n",
        "                        neg_reg1 = sum_rectangle_region(i,j,w,h)\n",
        "                        neg_reg2 = sum_rectangle_region(i,j+2*w,w,h)\n",
        "                        pos_reg = sum_rectangle_region(i,j+w,w,h)\n",
        "                        features_list.append([[pos_reg],[neg_reg1,neg_reg2]])\n",
        "                        \n",
        "                    ########### 4-rectangle Features\n",
        "                    \n",
        "                    if i+2*h<size and j+2*w<size:\n",
        "                        neg_reg1 = sum_rectangle_region(i,j,w,h)\n",
        "                        neg_reg2 = sum_rectangle_region(i+h,j+w,w,h)\n",
        "                        pos_reg1 = sum_rectangle_region(i,j+w,w,h)\n",
        "                        pos_reg2 = sum_rectangle_region(i+h,j,w,h)\n",
        "                        features_list.append([[pos_reg1,pos_reg2],[neg_reg1,neg_reg2]])\n",
        "                        \n",
        "                    w+=1\n",
        "                h+=1\n",
        "    return features_list\n",
        "\n",
        "###############\n",
        "\n",
        "def init_feature_image_matrix(features_list,integral_image_list, no_faces, no_nofaces): #Initialising various matrices\n",
        "\n",
        "  no_feats = len(features_list)\n",
        "  no_imgs = len(integral_image_list)\n",
        "  print(\"The number of features are %d and the number of images are %d\"%(no_feats,no_imgs))\n",
        "\n",
        "  feature_image = np.zeros((no_feats,no_imgs))\n",
        "  class_feature_image = np.zeros((1,no_imgs))\n",
        "  weights = np.zeros((1,no_imgs))\n",
        "  print(feature_image.shape)\n",
        "\n",
        "  for i in range(no_feats):\n",
        "    for j in range(no_imgs):\n",
        "      \n",
        "      feature_sum = 0\n",
        "      \n",
        "      for posregs in features_list[i][0]:\n",
        "          feature_sum+=posregs.compute_area(integral_image_list[j][0])\n",
        "          \n",
        "      for negregs in features_list[i][1]:\n",
        "          feature_sum-=negregs.compute_area(integral_image_list[j][0])\n",
        "      \n",
        "      class_feature_image[0,j] = integral_image_list[j][1]\n",
        "\n",
        "      if integral_image_list[j][1] == 0:\n",
        "        weights[0,j] = 1./(2*no_faces)\n",
        "      else:\n",
        "        weights[0,j] = 1./(2*no_nofaces)\n",
        "      feature_image[i,j] = feature_sum\n",
        "\n",
        "  return feature_image, weights, class_feature_image\n",
        "\n",
        "###############\n",
        "\n",
        "def select_best_classifier(): # Selecting the best classifier/feature\n",
        "  tpos_wgts = 0\n",
        "  tneg_wgts = 0\n",
        "\n",
        "  for i in range(classes_image.shape[1]): # Getting the total weights of each class\n",
        "    if classes_image[0,i] == 0:\n",
        "      tpos_wgts+= weights_image[0,i]\n",
        "    else:\n",
        "      tneg_wgts+= weights_image[0,i]\n",
        "  \n",
        "  best_feature_index = -1 # To store the best weak classifier index\n",
        "  feature_error = np.inf # The best weak classifier's error\n",
        "  best_polarity = 0\n",
        "  best_threshold = 0\n",
        "  error_array = np.zeros((1,feature_image_matrix.shape[1]))\n",
        "  start_time = time.time()\n",
        "\n",
        "  for i in range(feature_image_matrix.shape[0]): # Going through each feature\n",
        "\n",
        "    images_for_specific_feature = feature_image_matrix[i,:] # Feature value per image\n",
        "\n",
        "    total_set = zip(images_for_specific_feature, weights_image[0,:], classes_image[0,:])\n",
        "    sorted_total_set = sorted(total_set, key=lambda x: x[0])\n",
        "\n",
        "    error_final, positive_below, negative_below, psofar, nsofar, thresh, polarity= np.inf, 0, 0, 0, 0, None, None # Initialising the values\n",
        "    \n",
        "    for fi, w, c in sorted_total_set:\n",
        "      \n",
        "      error = min(positive_below + tneg_wgts - negative_below, negative_below + tpos_wgts - positive_below)\n",
        "      # error = min(negative_below + tpos_wgts - positive_below, positive_below + tneg_wgts - negative_below)\n",
        "      if error<error_final:\n",
        "        error_final = error\n",
        "\n",
        "        thresh = fi\n",
        "        if psofar>nsofar:\n",
        "          polarity = 1\n",
        "        else:\n",
        "          polarity = -1\n",
        "\n",
        "      if int(c) == 0:\n",
        "        positive_below+=w\n",
        "        psofar+=1\n",
        "      else:\n",
        "        negative_below+=w\n",
        "        nsofar+=1\n",
        "\n",
        "    #  feat_error = np.sum(weights_image[0,:][polarity*images_for_specific_feature<polarity*thresh]) \n",
        "\n",
        "    feat_error = 0\n",
        "    errorarray_temp = np.zeros((1,feature_image_matrix.shape[1]))\n",
        "    for index, ii in enumerate(images_for_specific_feature):\n",
        "\n",
        "      if polarity*ii<polarity*thresh and classes_image[0,:][index] == 1:\n",
        "        feat_error+=weights_image[0,:][index]\n",
        "        errorarray_temp[0,index] = 1\n",
        "      elif polarity*ii>polarity*thresh and classes_image[0,:][index] == 0:\n",
        "        feat_error+=weights_image[0,:][index]\n",
        "        errorarray_temp[0,index] = 1\n",
        "\n",
        "    if feat_error<feature_error:\n",
        "      feature_error = feat_error\n",
        "      best_feature_index = i\n",
        "      error_array = errorarray_temp\n",
        "      best_polarity = polarity\n",
        "      best_threshold = thresh\n",
        "\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "  return feature_error, best_feature_index, error_array, best_polarity, best_threshold\n",
        "\n",
        "########\n",
        "\n",
        "def test_algo(integral_image_list,final_best_classifiers,polarity_list,threshold_list,beta_list): #Code to test the accuracy\n",
        "  count = 0\n",
        "\n",
        "  for index1, j in enumerate(integral_image_list):\n",
        "    final_value = 0\n",
        "    alpha_sum = 0\n",
        "    for index,i in enumerate(final_best_classifiers):\n",
        "\n",
        "      feature_sum = 0\n",
        "    \n",
        "      for posregs in features_list[i][0]:\n",
        "          feature_sum+=posregs.compute_area(j[0])\n",
        "          \n",
        "      for negregs in features_list[i][1]:\n",
        "          feature_sum-=negregs.compute_area(j[0])\n",
        "\n",
        "      polarity = polarity_list[index]\n",
        "      threshold = threshold_list[index]\n",
        "      alpha_sum+= np.log(1/beta_list[index])\n",
        "\n",
        "      if polarity*feature_sum<polarity*threshold:\n",
        "          final_value+= np.log(1/beta_list[index])\n",
        "  \n",
        "\n",
        "    if final_value >= 0.5*alpha_sum and classes_image[0,index1] == 0:\n",
        "      count+=1\n",
        "    elif final_value < 0.5*alpha_sum and classes_image[0,index1] == 1:\n",
        "      count+=1\n",
        "\n",
        "  return count"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY6wKlXbbAeG"
      },
      "source": [
        "# Code to \n",
        "\n",
        "1. Get the images.\n",
        "2. Divide them into train/test.\n",
        "3. Generate the integral images.\n",
        "4. Initialise various matrices which we need in our code.\n",
        "\n",
        "# Do not run unless you want to retrain the entire model (will take 2-3 hours)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV4ssjCnd1hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de6ab32-3e8e-44fd-c4a6-fdae65a67333"
      },
      "source": [
        "### Do not run unless you want to train from scratch\n",
        "\n",
        "size = 9\n",
        "\n",
        "mypath_face = \"/content/drive/MyDrive/faces/face.train/face/train/face/\"\n",
        "mypath_noface = \"/content/drive/MyDrive/faces/face.train/face/train/non-face/\"\n",
        "\n",
        "onlyfiles_face = [f for f in listdir(mypath_face) if isfile(join(mypath_face, f))][:1000]\n",
        "onlyfiles_noface = [f for f in listdir(mypath_noface) if isfile(join(mypath_noface, f))][:1000]\n",
        "\n",
        "random.shuffle(onlyfiles_face)\n",
        "random.shuffle(onlyfiles_noface)\n",
        "\n",
        "train_onlyfiles_face = onlyfiles_face[:900]\n",
        "train_onlyfiles_noface = onlyfiles_noface[:900]\n",
        "\n",
        "test_onlyfiles_face = onlyfiles_face[900:]\n",
        "test_onlyfiles_noface = onlyfiles_noface[900:]\n",
        "\n",
        "\n",
        "no_faces = len(train_onlyfiles_face)\n",
        "no_nofaces = len(train_onlyfiles_noface)\n",
        "no_totalfaces = no_faces + no_nofaces\n",
        "\n",
        "print(\"There are %d training images with faces\"%(len(train_onlyfiles_face)))\n",
        "print(\"There are %d training images with no faces\"%(len(train_onlyfiles_noface)))\n",
        "integral_image_list = []\n",
        "\n",
        "count = 0\n",
        "for f in train_onlyfiles_face:\n",
        "  image_file = mypath_face + f\n",
        "  img = cv2.imread(image_file,0)\n",
        "  img = cv2.resize(img,(size,size))\n",
        "  integral_image_list.append([integral_image(img),0])\n",
        "  count+=1\n",
        "  if count%100 == 0:\n",
        "    print(count)\n",
        "\n",
        "count = 0\n",
        "for f in train_onlyfiles_noface:\n",
        "  image_file = mypath_noface + f\n",
        "  img = cv2.imread(image_file,0)\n",
        "  img = cv2.resize(img,(size,size))\n",
        "  integral_image_list.append([integral_image(img),1])\n",
        "  count+=1\n",
        "  if count%100 == 0:\n",
        "    print(count)\n",
        "\n",
        "features_list = features(size)\n",
        "start_time = time.time()\n",
        "feature_image_matrix, weights_image, classes_image = init_feature_image_matrix(features_list,integral_image_list, no_faces, no_nofaces)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 900 training images with faces\n",
            "There are 900 training images with no faces\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "The number of features are 2056 and the number of images are 1800\n",
            "(2056, 1800)\n",
            "--- 31.600548267364502 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY35-1R8bmLa"
      },
      "source": [
        "# Code to save all the matrices so that we don't have to retrain everytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UujEHdqYSSpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be3e491-0aff-4573-fb4b-afdbd42274af"
      },
      "source": [
        "np.save( \"/content/drive/MyDrive/faces/feature_image.npy\",feature_image_matrix)\n",
        "np.save( \"/content/drive/MyDrive/faces/weights_image.npy\",weights_image)\n",
        "np.save( \"/content/drive/MyDrive/faces/classes_image.npy\",classes_image)\n",
        "np.save( \"/content/drive/MyDrive/faces/integral_image.npy\",np.array(integral_image_list))\n",
        "np.save( \"/content/drive/MyDrive/faces/features.npy\",np.array(features_list))\n",
        "np.save( \"/content/drive/MyDrive/faces/test_face.npy\",np.array(test_onlyfiles_face))\n",
        "np.save( \"/content/drive/MyDrive/faces/test_noface.npy\",np.array(test_onlyfiles_noface))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhxEb6TNbrw4"
      },
      "source": [
        "# Code to open all the saved matrices so that we don't have to retrain everytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJXaW4eax5Mj"
      },
      "source": [
        "feature_image_matrix = np.load(\"/content/drive/MyDrive/faces/feature_image.npy\")\n",
        "weights_image = np.load( \"/content/drive/MyDrive/faces/weights_image.npy\")\n",
        "classes_image = np.load(\"/content/drive/MyDrive/faces/classes_image.npy\")\n",
        "integral_image_list = np.load(\"/content/drive/MyDrive/faces/integral_image.npy\",allow_pickle = True)\n",
        "features_list = np.load(\"/content/drive/MyDrive/faces/features.npy\",allow_pickle = True)\n",
        "test_onlyfiles_face = np.load(\"/content/drive/MyDrive/faces/test_face.npy\",allow_pickle = True)\n",
        "test_onlyfiles_noface =  np.load(\"/content/drive/MyDrive/faces/test_noface.npy\",allow_pickle = True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1SbAnubx8Q"
      },
      "source": [
        "# Code to get the best classifiers\n",
        "# Do not run unless you want to retrain the entire model (will take 2-3 hours)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXIoNxOF0p9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e295933c-9a61-44c8-a436-37850b69360e"
      },
      "source": [
        "# indices = SelectPercentile(f_classif, percentile=10).fit(feature_image_matrix.T,classes_image[0,:]).get_support(indices=True)\n",
        "# feature_image_matrix = feature_image_matrix[indices]\n",
        "# print(feature_image_matrix.shape)\n",
        "\n",
        "final_best_classifiers = []\n",
        "beta_list = []\n",
        "polarity_list = []\n",
        "threshold_list =[]\n",
        "T = 10\n",
        "for i in range(T):\n",
        "  print(i)\n",
        "  # weights_image = weights_image / np.linalg.norm(weights_image)\n",
        "  # feature_error, best_feature_index, error_array, best_polarity, best_threshold = select_best_classifier(feature_image_matrix, weights_image, classes_image)\n",
        "  feature_error, best_feature_index, error_array, best_polarity, best_threshold = select_best_classifier()\n",
        "  print(best_feature_index)\n",
        "  final_best_classifiers.append(best_feature_index)\n",
        "  polarity_list.append(best_polarity)\n",
        "  threshold_list.append(best_threshold)\n",
        "  beta = feature_error/(1-feature_error)\n",
        "  beta_list.append(beta)\n",
        "  for index,i in enumerate(weights_image[0,:]):\n",
        "    weights_image[0,index] = weights_image[0,index]*(beta**(1-(error_array[0,index])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "--- 16.82512640953064 seconds ---\n",
            "500\n",
            "1\n",
            "--- 16.38855218887329 seconds ---\n",
            "694\n",
            "2\n",
            "--- 17.10617184638977 seconds ---\n",
            "1132\n",
            "3\n",
            "--- 16.940529823303223 seconds ---\n",
            "423\n",
            "4\n",
            "--- 17.123265743255615 seconds ---\n",
            "445\n",
            "5\n",
            "--- 16.96337580680847 seconds ---\n",
            "381\n",
            "6\n",
            "--- 17.12041664123535 seconds ---\n",
            "354\n",
            "7\n",
            "--- 16.90467643737793 seconds ---\n",
            "423\n",
            "8\n",
            "--- 17.15509819984436 seconds ---\n",
            "445\n",
            "9\n",
            "--- 16.858633995056152 seconds ---\n",
            "429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXiWCgeMcCaI"
      },
      "source": [
        "# Code to save all the matrices so that we don't have to retrain everytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF2adw-wZp5J"
      },
      "source": [
        "np.save( \"/content/drive/MyDrive/faces/final_best_classifiers.npy\",final_best_classifiers)\n",
        "np.save( \"/content/drive/MyDrive/faces/polarity_list.npy\",polarity_list)\n",
        "np.save( \"/content/drive/MyDrive/faces/threshold_list.npy\",threshold_list)\n",
        "np.save( \"/content/drive/MyDrive/faces/beta_list.npy\",np.array(beta_list))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHKY0lxUcVRK"
      },
      "source": [
        "# Code to open all the saved matrices so that we don't have to retrain everytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfalUqsqcFDL"
      },
      "source": [
        "final_best_classifiers = np.load(\"/content/drive/MyDrive/faces/final_best_classifiers.npy\")\n",
        "polarity_list = np.load(\"/content/drive/MyDrive/faces/polarity_list.npy\")\n",
        "threshold_list =  np.load(\"/content/drive/MyDrive/faces/threshold_list.npy\")\n",
        "beta_list = np.load(\"/content/drive/MyDrive/faces/beta_list.npy\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVDdJCLAcXNi"
      },
      "source": [
        "# Training Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS3KI_bKZYxl",
        "outputId": "143bc97b-8393-4061-af5c-f501c04f942f"
      },
      "source": [
        "count = test_algo(integral_image_list,final_best_classifiers,polarity_list,threshold_list,beta_list)\n",
        "print(no_totalfaces)\n",
        "print(\"The accuracy of the model is: %f\"%(count/1800))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "The accuracy of the model is: 0.593333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLghRVX9cd1e"
      },
      "source": [
        "# Testing Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buwtdNBjGr1x",
        "outputId": "33b42fcd-568f-4c7e-b906-786ca3d1146a"
      },
      "source": [
        "### reading the image\n",
        "\n",
        "# mypath_face = \"/content/drive/MyDrive/faces/face.test/face/test/face/\"\n",
        "# mypath_noface = \"/content/drive/MyDrive/faces/face.test/face/test/non-face/\"\n",
        "\n",
        "mypath_face = \"/content/drive/MyDrive/faces/face.train/face/train/face/\"\n",
        "mypath_noface = \"/content/drive/MyDrive/faces/face.train/face/train/non-face/\"\n",
        "\n",
        "# test_onlyfiles_face = []\n",
        "# test_onlyfiles_noface = []\n",
        "# test_onlyfiles_face = [f for f in listdir(mypath_face) if isfile(join(mypath_face, f))]\n",
        "# test_onlyfiles_noface = [f for f in listdir(mypath_noface) if isfile(join(mypath_noface, f))][:200]\n",
        "\n",
        "no_faces = len(test_onlyfiles_face)\n",
        "no_nofaces = len(test_onlyfiles_noface)\n",
        "no_totalfaces = no_faces + no_nofaces\n",
        "\n",
        "classes_image = np.zeros((1,no_totalfaces))\n",
        "classes_image[0,no_faces:] = 1\n",
        "\n",
        "integral_image_list = []\n",
        "\n",
        "count = 0\n",
        "for f in test_onlyfiles_face:\n",
        "  image_file = mypath_face + f\n",
        "  img = cv2.imread(image_file,0)\n",
        "  # img = cv2.resize(img,(size,size))\n",
        "  integral_image_list.append([integral_image(img),0])\n",
        "\n",
        "count = 0\n",
        "for f in test_onlyfiles_noface:\n",
        "  image_file = mypath_noface + f\n",
        "  img = cv2.imread(image_file,0)\n",
        "  # img = cv2.resize(img,(size,size))\n",
        "  integral_image_list.append([integral_image(img),1])\n",
        "\n",
        "count = test_algo(integral_image_list,final_best_classifiers,polarity_list,threshold_list,beta_list)\n",
        "print(no_totalfaces)\n",
        "print(\"The accuracy of the model is: %f\"%(count/no_totalfaces))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "The accuracy of the model is: 0.522222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLPDwEwMeBgX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}